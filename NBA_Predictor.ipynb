{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOoGN8X5jku6fTILkCWpkUI"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Scraping NBA MVPs from Basketball-Reference"
      ],
      "metadata": {
        "id": "Idozzg1Echn4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UBgxsqvRAc68"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import time\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating list of ranges from 1991 - 2021\n",
        "<br /> Needs to be seperated for request volume limit of 30"
      ],
      "metadata": {
        "id": "0lC65Q8Xc943"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "years = list(range(1991, 2010))\n",
        "years2 = list(range(2010, 2022))\n",
        "\n",
        "year_all = list(range(1991, 2022))"
      ],
      "metadata": {
        "id": "K85TB_NfBJcn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Getting url for website to scrap, using {} so it can be looped"
      ],
      "metadata": {
        "id": "S2kEvWc6d3U8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url_start = \"https://www.basketball-reference.com/awards/awards_{}.html\""
      ],
      "metadata": {
        "id": "-8VA2IXsBN1A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looping over both ranges once to get data stored in a folder using requests and writes"
      ],
      "metadata": {
        "id": "9ReA9xx3eHLj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for year in years2:\n",
        "  url = url_start.format(year) #replaces {} with the year in loop\n",
        "  data = requests.get(url) #getting html data from the page\n",
        "\n",
        "  with open(\"mvp/{}.html\".format(year), \"w+\") as f: #creates a file in the folder then is given writing permission\n",
        "    f.write(data.text) #writes html into file"
      ],
      "metadata": {
        "id": "qMR_7lLoBUq_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looping every table and storing them in a dataframes after parsing and finding the tables. Add a year column to keep track of which table it is"
      ],
      "metadata": {
        "id": "k1CrfxVufbgc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mvp_all = [] #initialises dataframe\n",
        "for year in year_all:\n",
        "  with open(\"mvp/{}.html\".format(year)) as f: #looks at file but only for reading (by default)\n",
        "    page = f.read()\n",
        "  soup = BeautifulSoup(page, \"html.parser\") #using BeautifulSoup to parse html\n",
        "  soup.find(\"tr\", class_=\"over_header\").decompose() #removing the header of the table\n",
        "  mvp_table = soup.find(id=\"mvp\") #finds the mvp table and puts it in a list\n",
        "  mvp = pd.read_html(str(mvp_table))[0] #Get the first table in the list of dataframes\n",
        "  mvp[\"Year\"] = year\n",
        "\n",
        "  mvp_all.append(mvp)"
      ],
      "metadata": {
        "id": "3JzmwEp_I9l2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mvps = pd.concat(mvp_all)"
      ],
      "metadata": {
        "id": "2qxVHgyUJKpH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mvps.to_csv(\"mvps.csv\")"
      ],
      "metadata": {
        "id": "c2vxTHLGTlru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Selenium does not work with google colab, so code will work only locally or with Jupyter"
      ],
      "metadata": {
        "id": "a-0lfvnb4mAh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Simple selenium use of opening web page up first to execute all javascript then take the pages source"
      ],
      "metadata": {
        "id": "SgOBdNbY_Tq5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "driver = webdriver.Chrome() #Starts driver\n",
        "\n",
        "player_stats_url = \"https://www.basketball-reference.com/leagues/NBA_{}_per_game.html\"\n",
        "\n",
        "for year in year_all:\n",
        "  url = player_stats_url.format(year)\n",
        "\n",
        "  driver.get(url) #driver opens page\n",
        "  driver.execute_script(\"window.scrollTo(1,1000)\") #scrolls all the way down to execute all javascript\n",
        "\n",
        "  time.sleep(2)\n",
        "  html = driver.page_source #Gets all the tables instead of only a few\n",
        "  with open(\"player/{}.html\".format(year), \"w+\") as f:\n",
        "    f.write(html)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "L_rMyPAfmFcT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remove the headers in-between and append into dataframe, same as before"
      ],
      "metadata": {
        "id": "fTgrdizk_fGA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "df = [] #initialises dataframe\n",
        "for year in year_all:\n",
        "  with open(\"player/{}.html\".format(year)) as f:\n",
        "    page = f.read()\n",
        "  soup = BeautifulSoup(page, \"html.parser\") #using BeautifulSoup to parse html\n",
        "  soup.find(\"tr\", class_=\"thead\").decompose() #removing the header of the table\n",
        "  player_table = soup.find(id=\"per_game_stats\") #finds the mvp table and puts it in a list\n",
        "  player = pd.read_html(str(player_table))[0] #Get the first table in the list of dataframes\n",
        "  player[\"Year\"] = year\n",
        "\n",
        "  df.append(player)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "c9AQ1BxwscpH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "players = pd.concat(df)\n",
        "players.to_csv(\"players.csv\")\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "Xoq5OtgUDAE6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "team_stats_url = \"https://www.basketball-reference.com/leagues/NBA_{}_standings.html\"\n",
        "\n",
        "for year in years2:\n",
        "  url = team_stats_url.format(year)\n",
        "\n",
        "  data = requests.get(url)\n",
        "\n",
        "  with open(\"team/{}.html\".format(year), \"w+\") as f:\n",
        "      f.write(data.text)"
      ],
      "metadata": {
        "id": "mkW584sjEQqZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = []\n",
        "for year in year_all:\n",
        "  with open(\"team/{}.html\".format(year)) as f:\n",
        "    page = f.read()\n",
        "  soup = BeautifulSoup(page, \"html.parser\")\n",
        "  soup.find(\"tr\", class_=\"thead\").decompose()\n",
        "  team_table = soup.find_all(id=\"divs_standings_E\")[0]\n",
        "  team = pd.read_html(str(team_table))[0]\n",
        "  team[\"Year\"] = year\n",
        "  team[\"Team\"] = team[\"Eastern Conference\"]\n",
        "  del team[\"Eastern Conference\"]\n",
        "\n",
        "  df.append(team)\n",
        "\n",
        "  team_table = soup.find_all(id=\"divs_standings_W\") [0]\n",
        "  team = pd.read_html(str(team_table))[0]\n",
        "  team[\"Year\"] = year\n",
        "  team[\"Team\"] = team[\"Western Conference\"]\n",
        "  del team[\"Western Conference\"]\n",
        "\n",
        "  df.append(team)"
      ],
      "metadata": {
        "id": "G0c0j0gWUkMa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "teams = pd.concat(df)"
      ],
      "metadata": {
        "id": "lZFqxM9bZ69B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "teams.to_csv(\"teams.csv\")"
      ],
      "metadata": {
        "id": "jaE1UTvOaCM7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}